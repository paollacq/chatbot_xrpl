{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be9538f-d84a-4040-bee1-5cc94815767f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2245cdc6b0bb4d77855047d94f9845ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today? (Type 'exit' or 'bye' to leave)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Which course is recommended for beginners?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e28e1f999540d0ab177ae35958063a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Course 1: Blockchain Fundamentals\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:   How much does it cost?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f835952c23c4ace9730057cedbccc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Original Price: $50.00  Discounted Price: $45.00\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how to make payment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: You can make payments using the chatbot interface following these steps...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Thank you for chatting with me. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"chatbot_logs.txt\", level=logging.INFO)\n",
    "\n",
    "# FAQs\n",
    "FAQS = {\n",
    "    \"what is xrp?\": \"XRP is a cryptocurrency developed by Ripple Labs for real-time global payments.\",\n",
    "    \"how to make payment?\": \"You can make payments using the chatbot interface following these steps...\"\n",
    "}\n",
    "\n",
    "# 1. Load and validate fragments\n",
    "def load_fragments(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            fragments = pickle.load(f)\n",
    "            if not fragments:\n",
    "                raise ValueError(\"No fragments found.\")\n",
    "            return fragments\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: Fragments file not found at {file_path}\")\n",
    "\n",
    "fragments = load_fragments(\"../data/fragments.pkl\")\n",
    "\n",
    "# 2. Create vectorstore\n",
    "embedding_model = \"thenlper/gte-large\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=fragments,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./vectorstore\"\n",
    ")\n",
    "\n",
    "# 3. Load Hugging Face model and pipeline\n",
    "llm_model_name = \"google/flan-t5-large\" #\"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# 4. Configure memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 5. Create QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# 6. Main chatbot function\n",
    "def chat_with_user():\n",
    "    print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' or 'bye' to leave)\")\n",
    "    while True:\n",
    "        query = input(\"You: \").strip().lower()\n",
    "        \n",
    "        if query in [\"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Thank you for chatting with me. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query in FAQS:\n",
    "            response = FAQS[query]\n",
    "        else:\n",
    "            try:\n",
    "                result = qa_chain({\"question\": query})\n",
    "                response = result.get(\"answer\", \"I'm sorry, I couldn't find an answer.\")\n",
    "            except Exception as e:\n",
    "                response = f\"Chatbot: An error occurred. Please try again later. Error: {e}\"\n",
    "        \n",
    "        print(f\"Chatbot: {response}\")\n",
    "        logging.info(f\"User Query: {query}\")\n",
    "        logging.info(f\"Chatbot Response: {response}\")\n",
    "\n",
    "# 7. Start chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25f57c-a888-4b88-a9c3-3f04bbb6119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb94e0-1f98-4d0d-81f7-777dd70b0002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
