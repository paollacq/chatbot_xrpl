{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9538f-d84a-4040-bee1-5cc94815767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63068/427200220.py:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
      "/tmp/ipykernel_63068/427200220.py:46: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
      "/tmp/ipykernel_63068/427200220.py:49: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today? (Type 'exit' or 'bye' to leave)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63068/427200220.py:72: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": query})\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Basic knowledge of financial markets\n",
      "Chatbot: Stock market\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals .\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: None\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: If you are just starting, we recommend Course 1: Blockchain Fundamentals\n",
      "Chatbot: None\n",
      "Chatbot: None\n",
      "Chatbot: None\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"chatbot_logs.txt\", level=logging.INFO)\n",
    "\n",
    "# FAQs\n",
    "FAQS = {\n",
    "    \"what is xrp?\": \"XRP is a cryptocurrency developed by Ripple Labs for real-time global payments.\",\n",
    "    \"how to make payment?\": \"You can make payments using the chatbot interface following these steps...\"\n",
    "}\n",
    "\n",
    "# 1. Load and validate fragments\n",
    "def load_fragments(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            fragments = pickle.load(f)\n",
    "            if not fragments:\n",
    "                raise ValueError(\"No fragments found.\")\n",
    "            return fragments\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: Fragments file not found at {file_path}\")\n",
    "\n",
    "fragments = load_fragments(\"../data/fragments.pkl\")\n",
    "\n",
    "# 2. Create vectorstore\n",
    "embedding_model = \"thenlper/gte-large\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=fragments,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./vectorstore\"\n",
    ")\n",
    "\n",
    "# 3. Load Hugging Face model and pipeline\n",
    "llm_model_name = \"google/flan-t5-large\" #\"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# 4. Configure memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 5. Create QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# 6. Main chatbot function\n",
    "def chat_with_user():\n",
    "    print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' or 'bye' to leave)\")\n",
    "    while True:\n",
    "        query = input(\"You: \").strip().lower()\n",
    "        \n",
    "        if query in [\"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Thank you for chatting with me. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query in FAQS:\n",
    "            response = FAQS[query]\n",
    "        else:\n",
    "            try:\n",
    "                result = qa_chain({\"question\": query})\n",
    "                response = result.get(\"answer\", \"I'm sorry, I couldn't find an answer.\")\n",
    "            except Exception as e:\n",
    "                response = f\"Chatbot: An error occurred. Please try again later. Error: {e}\"\n",
    "        \n",
    "        print(f\"Chatbot: {response}\")\n",
    "        logging.info(f\"User Query: {query}\")\n",
    "        logging.info(f\"Chatbot Response: {response}\")\n",
    "\n",
    "# 7. Start chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb94e0-1f98-4d0d-81f7-777dd70b0002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
